{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DG1fSr_B0O9"
   },
   "source": [
    "## **Gradient Vanishing and ResNet**\n",
    "\n",
    "This Colab file demonstrates the gradient vanishing problem and how ResNet can overcome the problem.\n",
    "\n",
    "Some of the code in this Colab file is based on the examples in https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7CIzWsA4GSI"
   },
   "outputs": [],
   "source": [
    "# Check version of Keras and Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print('Keras version:', keras.__version__)\n",
    "print('Tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NIKPeTWDltM"
   },
   "source": [
    "### **1. Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvViKfGrkE6K"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Create a working folder and cd to it.\n",
    "!mkdir -p /content/drive/MyDrive/Learning/EIE4122/lab3\n",
    "%cd /content/drive/MyDrive/Learning/EIE4122/lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3Glo-iXfnLG"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99I6fPZRpe2p"
   },
   "outputs": [],
   "source": [
    "# Create two 2D spirals\n",
    "import numpy as np\n",
    "def twospirals(n_points, noise=.5):\n",
    "    n = np.sqrt(np.random.rand(n_points,1)) * 600 * (2*np.pi)/360\n",
    "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
    "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
    "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))),\n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kih32ArYgYL4"
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = twospirals(500, noise=1.5)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJTCh9dcg4ag"
   },
   "outputs": [],
   "source": [
    "# Plot the two classes using two different colors\n",
    "from matplotlib import pyplot\n",
    "for i in range(2):\n",
    "  idx = (y == i)\n",
    "  pyplot.scatter(X[idx, 0], X[idx, 1], label=f\"Class {str(i)}\")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LTfbbNphfCj"
   },
   "outputs": [],
   "source": [
    "# Split into train and test, shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "n_train = y.shape[0]//2\n",
    "X, y = shuffle(X, y)\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83Gg-01vD6EC"
   },
   "source": [
    "### **2. Network definition and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpQzNDuPjr4d"
   },
   "outputs": [],
   "source": [
    "# Define and compile a feedforward net, using 'tanh' as non-linear activation function\n",
    "act = 'tanh'\n",
    "x_in = tf.keras.layers.Input((2,), name='Input')\n",
    "x = Dense(5, input_dim=2, activation=act, name='L1')(x_in)\n",
    "l = 1\n",
    "for i in range(30):\n",
    "  x = Dense(5, input_dim=5, activation=act, name=f'L{l+1}')(x)\n",
    "  l = l + 1\n",
    "x_out = Dense(1, activation='sigmoid', name='Output')(x)\n",
    "ffnet = tf.keras.models.Model(inputs=x_in, outputs=x_out, name=\"FFNet\")\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "ffnet.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#ffnet.summary()\n",
    "ffnet_init_weights = ffnet.get_weights()\n",
    "print(f\"No. of layers = {len(ffnet.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9abP3kQHUmC"
   },
   "outputs": [],
   "source": [
    "# Train the feedforward net without Tensorboard\n",
    "ffnet_hist = ffnet.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW-yYdcTHp5P"
   },
   "outputs": [],
   "source": [
    "# Evaluate the ffnet and plot training/test accuracy against epoch\n",
    "_, train_acc = ffnet.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = ffnet.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(ffnet_hist.history['accuracy'], label='train')\n",
    "pyplot.plot(ffnet_hist.history['val_accuracy'], label='test')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipBqEXvFTG9f"
   },
   "outputs": [],
   "source": [
    "# Define a ResNet, using 'tanh' as non-linear act function\n",
    "import tensorflow as tf\n",
    "from keras.layers import Add\n",
    "act = 'tanh'\n",
    "x_in = tf.keras.layers.Input((2,), name='Input')\n",
    "x = Dense(5, input_dim=2, activation=act, name='L1')(x_in)\n",
    "x = Dense(5, input_dim=5, activation=act, name='L2')(x)\n",
    "l = 3\n",
    "for i in range(25):\n",
    "  x_skip = x\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = Dense(5, input_dim=5, activation=act, name=f'L{l}')(x)\n",
    "  x = Dense(5, input_dim=5, activation=act, name=f'L{l+1}')(x)\n",
    "  x = Add()([x, x_skip])\n",
    "  l = l + 2\n",
    "x = Dense(5, input_dim=5, activation=act, name=f'L{l}')(x)\n",
    "x_out = Dense(1, activation='sigmoid', name='Output')(x)\n",
    "resnet = tf.keras.models.Model(inputs=x_in, outputs=x_out, name=\"ResNet\")\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "resnet.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#resnet.summary()\n",
    "resnet_init_weights = resnet.get_weights()\n",
    "print(f\"No. of layers = {len(resnet.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBaJ1Tb6nf8V"
   },
   "outputs": [],
   "source": [
    "# Train the resnet without Tensorboard\n",
    "resnet_hist = resnet.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jppayhoIlrG"
   },
   "outputs": [],
   "source": [
    "# Evaluate the resnet and plot training/test accuracy against epoch\n",
    "_, train_acc = resnet.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = resnet.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(resnet_hist.history['accuracy'], label='train')\n",
    "pyplot.plot(resnet_hist.history['val_accuracy'], label='test')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jonwZ-aKEKtl"
   },
   "source": [
    "### **3. Inspect the network using TensorBoard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "na-yyDkPxjIA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# keep track of the gradients using TensorBoard\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "class ExtendedTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "\n",
    "  def _log_gradients(self, epoch):\n",
    "    step = tf.cast(epoch, dtype=tf.int64)\n",
    "    writer = self._train_writer\n",
    "\n",
    "    with writer.as_default(), tf.GradientTape() as g:\n",
    "      y_pred = self.model(trainX)\n",
    "      y_true = np.asarray(trainy).astype('float32').reshape((-1,1))\n",
    "      loss = bce(y_true=y_true, y_pred=y_pred)\n",
    "      gradients = g.gradient(loss, self.model.trainable_weights) # back-propagation\n",
    "\n",
    "      # In \"eager\" mode, grads does not have name, so we get names from model.trainable_weights\n",
    "      # https://jonathan-hui.medium.com/tensorflow-eager-execution-v-s-graph-tf-function-6edaa870b1f1\n",
    "      for weights, grads in zip(self.model.trainable_weights, gradients):\n",
    "        tf.summary.histogram(weights.path.replace('/', '_')+'_grads', data=grads, step=step)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    # This function overrides on_epoch_end in tf.keras.callbacks.TensorBoard\n",
    "    # but we do need to run the original on_epoch_end, so here we use the super function.\n",
    "    super(ExtendedTensorBoard, self).on_epoch_end(epoch, logs=logs)\n",
    "    if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
    "      self._log_gradients(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg0oi_qLxUqz"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ffnet # Delete the logs from the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPik6rc3kRoE"
   },
   "outputs": [],
   "source": [
    "# Train the ffnet and use TensorBoard callback to log training info\n",
    "ffnet_tb = ExtendedTensorBoard(log_dir='./logs/ffnet', histogram_freq=1) # you can change the file name to store info for different models\n",
    "ffnet.set_weights(ffnet_init_weights)\n",
    "ffnet.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0, callbacks=[ffnet_tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ_x2d49Omhy"
   },
   "source": [
    "Inspect the **HISTOGRAM** in the TensorBoard to see how the weights in the bottom layers were changed during the training. In the histograms, the x-axis is the weight values, the y-axis is the epoch number, and the z-axis is the frequency of occurrences of the weight values.\n",
    "\n",
    "Inspect the **DISTRIBUTION** in the TensorBoard to see how the distribution of weights and gradients at different layers evolves during training. In the figures, the x-axis is the epoch, and the y-axis is the weight or gradient values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6jnB1h1p2xY"
   },
   "outputs": [],
   "source": [
    "# Use Tensorboard to inspect the training info and gradients\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir='./logs/ffnet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B20rbOddLb5x"
   },
   "outputs": [],
   "source": [
    "# Delete logs from previous runs\n",
    "!rm -rf ./logs/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYnAiNPOxv9u"
   },
   "outputs": [],
   "source": [
    "# Train the resent and use TensorBoard callback to log training info\n",
    "resnet_tb = ExtendedTensorBoard(log_dir='./logs/resnet', histogram_freq=1)\n",
    "resnet.set_weights(resnet_init_weights)\n",
    "resnet.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0, callbacks=[resnet_tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOjapvTIL0Uj"
   },
   "outputs": [],
   "source": [
    "# Use Tensorboard to inspect the training info and gradients\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir='./logs/resnet/' --port=8008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-vice60MBkB"
   },
   "source": [
    "### **<font color=\"red\">4. Further investigations</font>**\n",
    "\n",
    "1.   Change the number of layers in *ffnet* to find the maximum number of layers that a feedforward network can have without suffering from the vanishing gradient problem.\n",
    "2.   Change the number of layers in *resnet* to find the maximum number of layers that a residual network can have without suffering from vanishing gradient problem.\n",
    "3.   Change the 'tanh' to 'relu' and repeat (1) and (2) to see if ReLU can help mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka5IOKHxQM2r"
   },
   "source": [
    "### **<font color=\"red\">5. What to include in your report</font>**\n",
    "1.   Your Discussion/observations on the \"further investigations\" suggested in Section 4.\n",
    "2.   The screenshots of the histograms of the weights at the bottom and the upper layers.\n",
    "3.   An illustration of the structure of the ResNet (you can infer this from the code or use resnet.summary()).\n",
    "4.   An explanation of why the ResNet can be very deep without suffering from the vanishing gradient problem.\n",
    "5.   An explanation of why ReLU can help mitigating the vanishing gradient problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azQCNh0jevH9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
